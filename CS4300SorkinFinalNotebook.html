<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>bdb87a867db742329ebcb8dc7c35c486</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="9WNwNxx-vLWh">
<p>#4300 Final Project Final Version Harley Sorkin 5/8/2023</p>
<p>#Walmart Sales Predictor Description: Predict the weekly sales of a
given Walmart based on relevant information, including past weekly
sales, if there was a holiday that week, average local temperature, fuel
price, consumer price index, and unemployment rates.</p>
<p>This is a model that can hopefully be adapted to fit all retail based
businesses, allowing for more precise inventory management, staffing,
pay rates, and much more.</p>
<p>The dataset used can be found at: <a
href="https://www.kaggle.com/datasets/yasserh/walmart-dataset?resource=download"
class="uri">https://www.kaggle.com/datasets/yasserh/walmart-dataset?resource=download</a></p>
</div>
<div class="cell code" data-execution_count="2" id="tq2impshlw2Y">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> ModelCheckpoint</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Only used for calculating Mean Absolute Error (MAE)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;Walmart.csv&#39;</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>dfn <span class="op">=</span> pd.read_csv(<span class="st">&#39;Walmart.csv&#39;</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>dfz <span class="op">=</span> pd.read_csv(<span class="st">&#39;Walmart.csv&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:580}"
id="TCVxHYBruTzP" data-outputId="7168303b-aed7-41f4-9674-a902d8110926">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.dtypes)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Store             int64
Date             object
Weekly_Sales    float64
Holiday_Flag      int64
Temperature     float64
Fuel_Price      float64
CPI             float64
Unemployment    float64
dtype: object
</code></pre>
</div>
<div class="output execute_result" data-execution_count="3">

  <div id="df-0d9797e7-a174-4ff6-95af-d4a9762d42f5">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>Date</th>
      <th>Weekly_Sales</th>
      <th>Holiday_Flag</th>
      <th>Temperature</th>
      <th>Fuel_Price</th>
      <th>CPI</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>05-02-2010</td>
      <td>1643690.90</td>
      <td>0</td>
      <td>42.31</td>
      <td>2.572</td>
      <td>211.096358</td>
      <td>8.106</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>12-02-2010</td>
      <td>1641957.44</td>
      <td>1</td>
      <td>38.51</td>
      <td>2.548</td>
      <td>211.242170</td>
      <td>8.106</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>19-02-2010</td>
      <td>1611968.17</td>
      <td>0</td>
      <td>39.93</td>
      <td>2.514</td>
      <td>211.289143</td>
      <td>8.106</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>26-02-2010</td>
      <td>1409727.59</td>
      <td>0</td>
      <td>46.63</td>
      <td>2.561</td>
      <td>211.319643</td>
      <td>8.106</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>05-03-2010</td>
      <td>1554806.68</td>
      <td>0</td>
      <td>46.50</td>
      <td>2.625</td>
      <td>211.350143</td>
      <td>8.106</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6430</th>
      <td>45</td>
      <td>28-09-2012</td>
      <td>713173.95</td>
      <td>0</td>
      <td>64.88</td>
      <td>3.997</td>
      <td>192.013558</td>
      <td>8.684</td>
    </tr>
    <tr>
      <th>6431</th>
      <td>45</td>
      <td>05-10-2012</td>
      <td>733455.07</td>
      <td>0</td>
      <td>64.89</td>
      <td>3.985</td>
      <td>192.170412</td>
      <td>8.667</td>
    </tr>
    <tr>
      <th>6432</th>
      <td>45</td>
      <td>12-10-2012</td>
      <td>734464.36</td>
      <td>0</td>
      <td>54.47</td>
      <td>4.000</td>
      <td>192.327265</td>
      <td>8.667</td>
    </tr>
    <tr>
      <th>6433</th>
      <td>45</td>
      <td>19-10-2012</td>
      <td>718125.53</td>
      <td>0</td>
      <td>56.47</td>
      <td>3.969</td>
      <td>192.330854</td>
      <td>8.667</td>
    </tr>
    <tr>
      <th>6434</th>
      <td>45</td>
      <td>26-10-2012</td>
      <td>760281.43</td>
      <td>0</td>
      <td>58.85</td>
      <td>3.882</td>
      <td>192.308899</td>
      <td>8.667</td>
    </tr>
  </tbody>
</table>
<p>6435 rows × 8 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-0d9797e7-a174-4ff6-95af-d4a9762d42f5')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-0d9797e7-a174-4ff6-95af-d4a9762d42f5 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-0d9797e7-a174-4ff6-95af-d4a9762d42f5');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="24"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:300}"
id="aTkg-OwG-oem" data-outputId="6e7fe9ef-a789-4053-c271-84369874dcb2">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.describe(percentiles<span class="op">=</span>[<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>])</span></code></pre></div>
<div class="output execute_result" data-execution_count="24">

  <div id="df-1fbdef73-8e4c-45af-9030-1ab51132fdb0">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>Weekly_Sales</th>
      <th>Holiday_Flag</th>
      <th>Temperature</th>
      <th>Fuel_Price</th>
      <th>CPI</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6435.000000</td>
      <td>6.435000e+03</td>
      <td>6435.000000</td>
      <td>6435.000000</td>
      <td>6435.000000</td>
      <td>6435.000000</td>
      <td>6435.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>23.000000</td>
      <td>1.046965e+06</td>
      <td>0.069930</td>
      <td>60.663782</td>
      <td>3.358607</td>
      <td>171.578394</td>
      <td>7.999151</td>
    </tr>
    <tr>
      <th>std</th>
      <td>12.988182</td>
      <td>5.643666e+05</td>
      <td>0.255049</td>
      <td>18.444933</td>
      <td>0.459020</td>
      <td>39.356712</td>
      <td>1.875885</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>2.099862e+05</td>
      <td>0.000000</td>
      <td>-2.060000</td>
      <td>2.472000</td>
      <td>126.064000</td>
      <td>3.879000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>12.000000</td>
      <td>5.533501e+05</td>
      <td>0.000000</td>
      <td>47.460000</td>
      <td>2.933000</td>
      <td>131.735000</td>
      <td>6.891000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>23.000000</td>
      <td>9.607460e+05</td>
      <td>0.000000</td>
      <td>62.670000</td>
      <td>3.445000</td>
      <td>182.616521</td>
      <td>7.874000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>34.000000</td>
      <td>1.420159e+06</td>
      <td>0.000000</td>
      <td>74.940000</td>
      <td>3.735000</td>
      <td>212.743293</td>
      <td>8.622000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>45.000000</td>
      <td>3.818686e+06</td>
      <td>1.000000</td>
      <td>100.140000</td>
      <td>4.468000</td>
      <td>227.232807</td>
      <td>14.313000</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1fbdef73-8e4c-45af-9030-1ab51132fdb0')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1fbdef73-8e4c-45af-9030-1ab51132fdb0 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1fbdef73-8e4c-45af-9030-1ab51132fdb0');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="4"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:807}"
id="st7ZYcDCa_yd" data-outputId="ea0db4b4-b6b0-44df-82a6-bef25f417f83">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the number of rows and columns for the grid</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>n_rows <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n_cols <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the subplots</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the 2D array of subplots into a 1D array</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the features to plot</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">&#39;Weekly_Sales&#39;</span>, <span class="st">&#39;Holiday_Flag&#39;</span>, <span class="st">&#39;Temperature&#39;</span>, <span class="st">&#39;Fuel_Price&#39;</span>, <span class="st">&#39;CPI&#39;</span>, <span class="st">&#39;Unemployment&#39;</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through each feature and plot it in a subplot</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, var <span class="kw">in</span> <span class="bu">enumerate</span>(features):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a histogram in the current subplot</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    axes[i].hist(df[var], edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, bins<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the title, x-label, and y-label for the current subplot</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(var)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xlabel(<span class="st">&#39;Value&#39;</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    axes[i].set_ylabel(<span class="st">&#39;Frequency&#39;</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the spacing between subplots and show the plot</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/ba12d5c76128489b45b06b0e19e62fe1fb4bf3f8.png" /></p>
</div>
</div>
<section id="normalize-data" class="cell markdown" id="0t3QfL2Trtim">
<h1>Normalize data</h1>
</section>
<div class="cell code" data-execution_count="5"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:424}"
id="Mb3wPGAH60Nm" data-outputId="0e92f643-6c67-4c59-850c-5806c3ede0b4">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Min-Max normalization</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Store the minimum and maximum values for the &quot;Weekly_Sales&quot; column before normalization</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y_min <span class="op">=</span> dfn[<span class="st">&quot;Weekly_Sales&quot;</span>].<span class="bu">min</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y_max <span class="op">=</span> dfn[<span class="st">&quot;Weekly_Sales&quot;</span>].<span class="bu">max</span>()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> features:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  var_data <span class="op">=</span> dfn[var]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="bu">min</span> <span class="op">=</span> var_data.<span class="bu">min</span>()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="bu">max</span> <span class="op">=</span> var_data.<span class="bu">max</span>()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  scaled <span class="op">=</span> (var_data <span class="op">-</span> <span class="bu">min</span>) <span class="op">/</span> (<span class="bu">max</span> <span class="op">-</span> <span class="bu">min</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  dfn[var] <span class="op">=</span> scaled</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale date to integer representing weeks since start</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>dfn[<span class="st">&#39;Date&#39;</span>] <span class="op">=</span> pd.to_datetime(dfn[<span class="st">&#39;Date&#39;</span>], <span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%d</span><span class="st">-%m-%Y&#39;</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>dfn[<span class="st">&#39;Date&#39;</span>] <span class="op">=</span> ((dfn[<span class="st">&#39;Date&#39;</span>] <span class="op">-</span> dfn[<span class="st">&#39;Date&#39;</span>].<span class="bu">min</span>()).dt.days <span class="op">/</span> <span class="dv">7</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>dfn</span></code></pre></div>
<div class="output execute_result" data-execution_count="5">

  <div id="df-120c39ce-651c-45f9-99ab-0ad961d9230c">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>Date</th>
      <th>Weekly_Sales</th>
      <th>Holiday_Flag</th>
      <th>Temperature</th>
      <th>Fuel_Price</th>
      <th>CPI</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.0</td>
      <td>0.397291</td>
      <td>0.0</td>
      <td>0.434149</td>
      <td>0.050100</td>
      <td>0.840500</td>
      <td>0.405118</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1.0</td>
      <td>0.396811</td>
      <td>1.0</td>
      <td>0.396967</td>
      <td>0.038076</td>
      <td>0.841941</td>
      <td>0.405118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2.0</td>
      <td>0.388501</td>
      <td>0.0</td>
      <td>0.410861</td>
      <td>0.021042</td>
      <td>0.842405</td>
      <td>0.405118</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>3.0</td>
      <td>0.332458</td>
      <td>0.0</td>
      <td>0.476419</td>
      <td>0.044589</td>
      <td>0.842707</td>
      <td>0.405118</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>4.0</td>
      <td>0.372661</td>
      <td>0.0</td>
      <td>0.475147</td>
      <td>0.076653</td>
      <td>0.843008</td>
      <td>0.405118</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6430</th>
      <td>45</td>
      <td>138.0</td>
      <td>0.139437</td>
      <td>0.0</td>
      <td>0.654990</td>
      <td>0.764028</td>
      <td>0.651876</td>
      <td>0.460514</td>
    </tr>
    <tr>
      <th>6431</th>
      <td>45</td>
      <td>139.0</td>
      <td>0.145057</td>
      <td>0.0</td>
      <td>0.655088</td>
      <td>0.758016</td>
      <td>0.653427</td>
      <td>0.458884</td>
    </tr>
    <tr>
      <th>6432</th>
      <td>45</td>
      <td>140.0</td>
      <td>0.145337</td>
      <td>0.0</td>
      <td>0.553131</td>
      <td>0.765531</td>
      <td>0.654977</td>
      <td>0.458884</td>
    </tr>
    <tr>
      <th>6433</th>
      <td>45</td>
      <td>141.0</td>
      <td>0.140810</td>
      <td>0.0</td>
      <td>0.572701</td>
      <td>0.750000</td>
      <td>0.655013</td>
      <td>0.458884</td>
    </tr>
    <tr>
      <th>6434</th>
      <td>45</td>
      <td>142.0</td>
      <td>0.152491</td>
      <td>0.0</td>
      <td>0.595988</td>
      <td>0.706413</td>
      <td>0.654796</td>
      <td>0.458884</td>
    </tr>
  </tbody>
</table>
<p>6435 rows × 8 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-120c39ce-651c-45f9-99ab-0ad961d9230c')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-120c39ce-651c-45f9-99ab-0ad961d9230c button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-120c39ce-651c-45f9-99ab-0ad961d9230c');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="6"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:424}"
id="W-KBoeDtWQDp" data-outputId="537e85d3-e180-4956-9b55-954cf6953dbd">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Z-score normalization</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Store the mean and standard deviation for the &quot;Weekly_Sales&quot; column before normalization</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>y_mean <span class="op">=</span> dfz[<span class="st">&quot;Weekly_Sales&quot;</span>].mean()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>y_std <span class="op">=</span> dfz[<span class="st">&quot;Weekly_Sales&quot;</span>].std()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> features:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  var_data <span class="op">=</span> dfz[var]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  mean <span class="op">=</span> var_data.mean()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  std <span class="op">=</span> var_data.std()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  standardized <span class="op">=</span> (var_data <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  dfz[var] <span class="op">=</span> standardized</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale date to integer representing weeks since start</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>dfz[<span class="st">&#39;Date&#39;</span>] <span class="op">=</span> pd.to_datetime(dfz[<span class="st">&#39;Date&#39;</span>], <span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%d</span><span class="st">-%m-%Y&#39;</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>dfz[<span class="st">&#39;Date&#39;</span>] <span class="op">=</span> ((dfz[<span class="st">&#39;Date&#39;</span>] <span class="op">-</span> dfz[<span class="st">&#39;Date&#39;</span>].<span class="bu">min</span>()).dt.days <span class="op">/</span> <span class="dv">7</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>dfz</span></code></pre></div>
<div class="output execute_result" data-execution_count="6">

  <div id="df-cba858c4-7249-4240-a69e-6fd20e5a9025">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>Date</th>
      <th>Weekly_Sales</th>
      <th>Holiday_Flag</th>
      <th>Temperature</th>
      <th>Fuel_Price</th>
      <th>CPI</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.0</td>
      <td>1.057338</td>
      <td>-0.274183</td>
      <td>-0.995058</td>
      <td>-1.713667</td>
      <td>1.004097</td>
      <td>0.056959</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1.0</td>
      <td>1.054266</td>
      <td>3.646633</td>
      <td>-1.201077</td>
      <td>-1.765952</td>
      <td>1.007802</td>
      <td>0.056959</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2.0</td>
      <td>1.001128</td>
      <td>-0.274183</td>
      <td>-1.124091</td>
      <td>-1.840023</td>
      <td>1.008996</td>
      <td>0.056959</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>3.0</td>
      <td>0.642778</td>
      <td>-0.274183</td>
      <td>-0.760848</td>
      <td>-1.737631</td>
      <td>1.009771</td>
      <td>0.056959</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>4.0</td>
      <td>0.899844</td>
      <td>-0.274183</td>
      <td>-0.767896</td>
      <td>-1.598203</td>
      <td>1.010546</td>
      <td>0.056959</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6430</th>
      <td>45</td>
      <td>138.0</td>
      <td>-0.591443</td>
      <td>-0.274183</td>
      <td>0.228584</td>
      <td>1.390775</td>
      <td>0.519229</td>
      <td>0.365080</td>
    </tr>
    <tr>
      <th>6431</th>
      <td>45</td>
      <td>139.0</td>
      <td>-0.555507</td>
      <td>-0.274183</td>
      <td>0.229126</td>
      <td>1.364632</td>
      <td>0.523215</td>
      <td>0.356018</td>
    </tr>
    <tr>
      <th>6432</th>
      <td>45</td>
      <td>140.0</td>
      <td>-0.553719</td>
      <td>-0.274183</td>
      <td>-0.335799</td>
      <td>1.397311</td>
      <td>0.527200</td>
      <td>0.356018</td>
    </tr>
    <tr>
      <th>6433</th>
      <td>45</td>
      <td>141.0</td>
      <td>-0.582670</td>
      <td>-0.274183</td>
      <td>-0.227368</td>
      <td>1.329776</td>
      <td>0.527292</td>
      <td>0.356018</td>
    </tr>
    <tr>
      <th>6434</th>
      <td>45</td>
      <td>142.0</td>
      <td>-0.507974</td>
      <td>-0.274183</td>
      <td>-0.098335</td>
      <td>1.140241</td>
      <td>0.526734</td>
      <td>0.356018</td>
    </tr>
  </tbody>
</table>
<p>6435 rows × 8 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-cba858c4-7249-4240-a69e-6fd20e5a9025')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-cba858c4-7249-4240-a69e-6fd20e5a9025 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-cba858c4-7249-4240-a69e-6fd20e5a9025');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="7"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="ErA4ylG34yUf" data-outputId="7c7ce34b-f4b5-4950-b16b-ab9109e1201d">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot date vs normalized feature values, with each graph representing a different store</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>stores <span class="op">=</span> dfn[<span class="st">&#39;Store&#39;</span>].unique()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> store <span class="kw">in</span> stores:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  store_data <span class="op">=</span> dfn[dfn[<span class="st">&#39;Store&#39;</span>] <span class="op">==</span> store]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  plt.figure()</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i, var <span class="kw">in</span> <span class="bu">enumerate</span>(features):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    var_data <span class="op">=</span> store_data[var]</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    date_data <span class="op">=</span> store_data[<span class="st">&#39;Date&#39;</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(date_data, var_data, label<span class="op">=</span>var, color<span class="op">=</span><span class="ss">f&#39;C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  plt.title(<span class="ss">f&#39;Input Features vs. Date for Store </span><span class="sc">{</span>store<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">&quot;Date&quot;</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">&quot;Value&quot;</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get the first, middle, and last dates for the current store</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>  n_dates <span class="op">=</span> <span class="bu">len</span>(date_data)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>  first_date <span class="op">=</span> date_data.iloc[<span class="dv">0</span>]</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>  middle_date <span class="op">=</span> date_data.iloc[n_dates <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>  last_date <span class="op">=</span> date_data.iloc[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Set the x-ticks to only show the first, middle, and last dates</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>  plt.xticks([first_date, middle_date, last_date])</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-7-7fed75ae5943&gt;:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure()
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/10436f4f427ed1f634ca502e68bae9821aefe8d9.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/f55034db9ccb80d902d2290ac0bfd3b2ebbe5ece.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/36e249d697cef53eb3c0a581290f4f85c60b2994.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/6e532b1f4c84138621d97765af44302d70173138.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4cb0e9df81a84cf84fe1fd3668fa17a97b029d4a.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4dfbe0377165d9a676f075787da728e39152b427.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/6f092447bbb02ad6c499d84b9dc1521eadf18187.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/c3ea405d867758b91df15769dbdbf340a1476697.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/e08229f73941b88695aee45853129beab4c44192.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/be4b4342108c1f9345bd58d6cdfe6186be7a17f0.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/012cf75efb59c1ff17d58a4c6f7aa5ba5427a9e8.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/0d049878789376cc03a54424203ac34a8d4e1049.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/70d34f5942faa7297fb5b04cf9be4df558039029.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/e915e7f81342c9fd6f02134318cae1df920c838e.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/616710740814f2edefa096f35560ae6203f0935f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/d2b2812e82094c5cca1f6a690294e2297365849b.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/2f5158ca6791b3da0d6a9a15c859b834cc8f423a.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/3b052cfbc3df817e0602341401f6b1f15f7353bf.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/a8b24c6fa538c3c4e6261432bc742e89303c80ce.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/d4fb4e08a247bc4c5140f3cbb3eca991f961789c.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b2c3dd3f56dadb35c3b26bf2f4af5bcf4919d3d8.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/5ed4a3cb56d69ec33ae910bf6258890e82d20154.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/805e698b9069b8eb955563b6e6c1e7b17a46634e.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/435d12664977b74e30c0ba11f44162cdf25c3761.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/3d942d18e8fefda4b64c3a54e8b7ad40eadf762f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/c7a4e81f087606dc957d4e4492fb523d3ed6fe03.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/2bb9056fa6c243f80b2157e4cc14acf87374fd2f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/12790e5792b15c11cd3ffc580acd8b119cfa2957.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/3be0fd290b70576bcd8701df5e50c560d8e45077.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4efde486f0a5df8c8a709d61950d3bf58e7cbb1f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/049c967e3841073ba3c09be66d7396ea86a76d18.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/54d200de82d3ec8f59ffd36986cc0858f98998b4.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/6f6d326d94369fa7ff88aa09a104b2ecf48cf701.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/0ba09fedf2188a9cd4b92c42864569f965010905.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4b373663037d6291dc99601e82f93342ae9e7e8d.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/f8f6e43f0feb70bef256fa50b63dfdc5e55f0074.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/c8c4bfaff1746554ede2ff1420456c6457d2768b.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/34d746d5935f1781047159fdd263fd7a51ee9a9d.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b2a090cd12d39e5c478637658dd8767e34bc0d49.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/86681e280900f953b08cb51eb4ea594da3042c5a.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/bbd498a16fd39c5ece71a9f1fe82e049c4c670f5.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/27065ab909786249df3f1c0c75bc07654a8a3f4a.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/be8472d2ead4071667e6a6dc7a6bda94b15b0b7b.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/03ec32a565e2533b2af8eb1d35513c4670f97239.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/da02a3f1fb8ce12995d88609f19183988750da90.png" /></p>
</div>
</div>
<section id="overfit-model" class="cell markdown" id="ZnRWUpTUrn4B">
<h1>Overfit model</h1>
</section>
<div class="cell code" data-execution_count="20"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="oP681MsVcptW" data-outputId="6a97a5f6-fde2-425c-e63b-00fd4fb6039b">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Z-score</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dfz.drop([<span class="st">&quot;Weekly_Sales&quot;</span>, <span class="st">&quot;Date&quot;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dfz[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model architecture</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X.columns)]),</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">1</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_absolute_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X, y, epochs<span class="op">=</span><span class="dv">1000</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predictions</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>y_pred_orig <span class="op">=</span> (y_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>y_orig <span class="op">=</span> (y <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y_orig, y_pred_orig)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model using the same dataset</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> model.evaluate(X, y, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean absolute error: </span><span class="sc">{</span>mse<span class="sc">}</span><span class="ss">, Mean absolute error (original): </span><span class="sc">{</span>mae<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.7491
Epoch 2/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.6486
Epoch 3/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.6210
Epoch 4/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.5993
Epoch 5/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.5717
Epoch 6/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.5556
Epoch 7/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.5284
Epoch 8/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.5043
Epoch 9/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.4756
Epoch 10/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.4414
Epoch 11/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.4215
Epoch 12/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.4096
Epoch 13/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.4022
Epoch 14/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.3904
Epoch 15/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3736
Epoch 16/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3699
Epoch 17/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3624
Epoch 18/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3602
Epoch 19/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3434
Epoch 20/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3542
Epoch 21/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3384
Epoch 22/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3211
Epoch 23/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3205
Epoch 24/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.3113
Epoch 25/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.3147
Epoch 26/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.3069
Epoch 27/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.3063
Epoch 28/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2981
Epoch 29/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2988
Epoch 30/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2862
Epoch 31/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2803
Epoch 32/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2823
Epoch 33/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2723
Epoch 34/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2714
Epoch 35/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.2672
Epoch 36/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2867
Epoch 37/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.2707
Epoch 38/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2671
Epoch 39/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2648
Epoch 40/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2569
Epoch 41/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2516
Epoch 42/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2572
Epoch 43/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2492
Epoch 44/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2393
Epoch 45/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2538
Epoch 46/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2444
Epoch 47/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2404
Epoch 48/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.2364
Epoch 49/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2414
Epoch 50/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2897
Epoch 51/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2438
Epoch 52/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2362
Epoch 53/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2349
Epoch 54/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2305
Epoch 55/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2285
Epoch 56/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2350
Epoch 57/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2370
Epoch 58/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2272
Epoch 59/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.2422
Epoch 60/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2200
Epoch 61/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2303
Epoch 62/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2183
Epoch 63/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2249
Epoch 64/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2303
Epoch 65/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2194
Epoch 66/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2198
Epoch 67/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2129
Epoch 68/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2059
Epoch 69/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.2138
Epoch 70/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2289
Epoch 71/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2084
Epoch 72/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2021
Epoch 73/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2050
Epoch 74/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2047
Epoch 75/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2137
Epoch 76/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2076
Epoch 77/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2162
Epoch 78/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1995
Epoch 79/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2033
Epoch 80/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.2092
Epoch 81/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2030
Epoch 82/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2027
Epoch 83/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2036
Epoch 84/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2087
Epoch 85/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2048
Epoch 86/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2011
Epoch 87/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1952
Epoch 88/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1927
Epoch 89/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2032
Epoch 90/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1971
Epoch 91/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2156
Epoch 92/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.2019
Epoch 93/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.2067
Epoch 94/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2367
Epoch 95/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2223
Epoch 96/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2149
Epoch 97/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2036
Epoch 98/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2070
Epoch 99/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2018
Epoch 100/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2035
Epoch 101/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2043
Epoch 102/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1952
Epoch 103/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1834
Epoch 104/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1863
Epoch 105/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1784
Epoch 106/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1822
Epoch 107/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1759
Epoch 108/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1627
Epoch 109/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1736
Epoch 110/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1777
Epoch 111/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1744
Epoch 112/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1699
Epoch 113/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1686
Epoch 114/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1833
Epoch 115/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1789
Epoch 116/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1716
Epoch 117/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1620
Epoch 118/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1946
Epoch 119/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1663
Epoch 120/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1635
Epoch 121/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1602
Epoch 122/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1958
Epoch 123/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1672
Epoch 124/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1546
Epoch 125/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1797
Epoch 126/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1791
Epoch 127/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1860
Epoch 128/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1771
Epoch 129/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1477
Epoch 130/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1705
Epoch 131/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1574
Epoch 132/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1678
Epoch 133/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1494
Epoch 134/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1537
Epoch 135/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1831
Epoch 136/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1884
Epoch 137/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1792
Epoch 138/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1524
Epoch 139/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1561
Epoch 140/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1842
Epoch 141/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1554
Epoch 142/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1520
Epoch 143/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1614
Epoch 144/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1508
Epoch 145/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1622
Epoch 146/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1531
Epoch 147/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1520
Epoch 148/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1461
Epoch 149/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1526
Epoch 150/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1512
Epoch 151/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1656
Epoch 152/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1574
Epoch 153/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1534
Epoch 154/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1626
Epoch 155/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1797
Epoch 156/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.2019
Epoch 157/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1486
Epoch 158/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1623
Epoch 159/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.1544
Epoch 160/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1665
Epoch 161/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1721
Epoch 162/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1425
Epoch 163/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1635
Epoch 164/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1567
Epoch 165/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1547
Epoch 166/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1483
Epoch 167/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1762
Epoch 168/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1395
Epoch 169/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1441
Epoch 170/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1594
Epoch 171/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1540
Epoch 172/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1362
Epoch 173/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1536
Epoch 174/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1785
Epoch 175/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1569
Epoch 176/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1441
Epoch 177/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1587
Epoch 178/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1528
Epoch 179/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1775
Epoch 180/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1597
Epoch 181/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1674
Epoch 182/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1726
Epoch 183/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1465
Epoch 184/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1489
Epoch 185/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1480
Epoch 186/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1671
Epoch 187/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1637
Epoch 188/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1452
Epoch 189/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1523
Epoch 190/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1454
Epoch 191/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1322
Epoch 192/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1546
Epoch 193/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2739
Epoch 194/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2057
Epoch 195/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1939
Epoch 196/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1892
Epoch 197/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1758
Epoch 198/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1665
Epoch 199/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1606
Epoch 200/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1712
Epoch 201/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1653
Epoch 202/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1568
Epoch 203/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1500
Epoch 204/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1700
Epoch 205/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1484
Epoch 206/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1531
Epoch 207/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1622
Epoch 208/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1451
Epoch 209/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1571
Epoch 210/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1512
Epoch 211/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1939
Epoch 212/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1653
Epoch 213/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1642
Epoch 214/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1529
Epoch 215/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1531
Epoch 216/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1423
Epoch 217/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1556
Epoch 218/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1599
Epoch 219/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1371
Epoch 220/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1411
Epoch 221/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1476
Epoch 222/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1640
Epoch 223/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1425
Epoch 224/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1551
Epoch 225/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1596
Epoch 226/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1418
Epoch 227/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1306
Epoch 228/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1466
Epoch 229/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1391
Epoch 230/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1372
Epoch 231/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1338
Epoch 232/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1291
Epoch 233/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1464
Epoch 234/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1601
Epoch 235/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1419
Epoch 236/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1326
Epoch 237/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1418
Epoch 238/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1566
Epoch 239/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1515
Epoch 240/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1359
Epoch 241/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1495
Epoch 242/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1627
Epoch 243/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1345
Epoch 244/1000
202/202 [==============================] - 2s 12ms/step - loss: 0.1481
Epoch 245/1000
202/202 [==============================] - 2s 11ms/step - loss: 0.1237
Epoch 246/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1321
Epoch 247/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1589
Epoch 248/1000
202/202 [==============================] - 2s 11ms/step - loss: 0.1470
Epoch 249/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1356
Epoch 250/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1302
Epoch 251/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1357
Epoch 252/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1596
Epoch 253/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1437
Epoch 254/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1560
Epoch 255/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1461
Epoch 256/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1313
Epoch 257/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1373
Epoch 258/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1284
Epoch 259/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1390
Epoch 260/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1527
Epoch 261/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1435
Epoch 262/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1273
Epoch 263/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1328
Epoch 264/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1483
Epoch 265/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1482
Epoch 266/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1357
Epoch 267/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1346
Epoch 268/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1531
Epoch 269/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1448
Epoch 270/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1553
Epoch 271/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1243
Epoch 272/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1486
Epoch 273/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1298
Epoch 274/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1440
Epoch 275/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1272
Epoch 276/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1304
Epoch 277/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1311
Epoch 278/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1516
Epoch 279/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1254
Epoch 280/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1361
Epoch 281/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1358
Epoch 282/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1179
Epoch 283/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1317
Epoch 284/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1480
Epoch 285/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1257
Epoch 286/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1298
Epoch 287/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1190
Epoch 288/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1305
Epoch 289/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1278
Epoch 290/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1307
Epoch 291/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1414
Epoch 292/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1576
Epoch 293/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1220
Epoch 294/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1591
Epoch 295/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.1453
Epoch 296/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1989
Epoch 297/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1649
Epoch 298/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1536
Epoch 299/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1566
Epoch 300/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1521
Epoch 301/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1526
Epoch 302/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1544
Epoch 303/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1632
Epoch 304/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1498
Epoch 305/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1493
Epoch 306/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1613
Epoch 307/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1514
Epoch 308/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1498
Epoch 309/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1894
Epoch 310/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1504
Epoch 311/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1432
Epoch 312/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1346
Epoch 313/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1374
Epoch 314/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1268
Epoch 315/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1325
Epoch 316/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1214
Epoch 317/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1309
Epoch 318/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1266
Epoch 319/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1321
Epoch 320/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1377
Epoch 321/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1222
Epoch 322/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1238
Epoch 323/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1217
Epoch 324/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1359
Epoch 325/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1230
Epoch 326/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1176
Epoch 327/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1165
Epoch 328/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1240
Epoch 329/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1211
Epoch 330/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1191
Epoch 331/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1198
Epoch 332/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1382
Epoch 333/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1269
Epoch 334/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1212
Epoch 335/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1272
Epoch 336/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1312
Epoch 337/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1197
Epoch 338/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1267
Epoch 339/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1321
Epoch 340/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1276
Epoch 341/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1301
Epoch 342/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1169
Epoch 343/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1211
Epoch 344/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1166
Epoch 345/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1319
Epoch 346/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1197
Epoch 347/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1155
Epoch 348/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1219
Epoch 349/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1357
Epoch 350/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1198
Epoch 351/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1304
Epoch 352/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1196
Epoch 353/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1234
Epoch 354/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1269
Epoch 355/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1142
Epoch 356/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1143
Epoch 357/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1191
Epoch 358/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1124
Epoch 359/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1247
Epoch 360/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1158
Epoch 361/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1231
Epoch 362/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1195
Epoch 363/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1198
Epoch 364/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1268
Epoch 365/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1223
Epoch 366/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1185
Epoch 367/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1113
Epoch 368/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1133
Epoch 369/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1273
Epoch 370/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1341
Epoch 371/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1183
Epoch 372/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1124
Epoch 373/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1168
Epoch 374/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1148
Epoch 375/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1133
Epoch 376/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1384
Epoch 377/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1408
Epoch 378/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1192
Epoch 379/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1175
Epoch 380/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1103
Epoch 381/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1164
Epoch 382/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1334
Epoch 383/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1301
Epoch 384/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1069
Epoch 385/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1130
Epoch 386/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1139
Epoch 387/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1243
Epoch 388/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1100
Epoch 389/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1074
Epoch 390/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1133
Epoch 391/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1360
Epoch 392/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1232
Epoch 393/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1239
Epoch 394/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1198
Epoch 395/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1113
Epoch 396/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1204
Epoch 397/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1284
Epoch 398/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.2027
Epoch 399/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1604
Epoch 400/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1428
Epoch 401/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1317
Epoch 402/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.1341
Epoch 403/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1213
Epoch 404/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1217
Epoch 405/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1243
Epoch 406/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1256
Epoch 407/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1182
Epoch 408/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1165
Epoch 409/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1249
Epoch 410/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1254
Epoch 411/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1199
Epoch 412/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1190
Epoch 413/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1188
Epoch 414/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1266
Epoch 415/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1246
Epoch 416/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1111
Epoch 417/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1094
Epoch 418/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1151
Epoch 419/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1065
Epoch 420/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1147
Epoch 421/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1115
Epoch 422/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1364
Epoch 423/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1158
Epoch 424/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1389
Epoch 425/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1156
Epoch 426/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1263
Epoch 427/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1058
Epoch 428/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1414
Epoch 429/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1102
Epoch 430/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1138
Epoch 431/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1117
Epoch 432/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1292
Epoch 433/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1331
Epoch 434/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1327
Epoch 435/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1282
Epoch 436/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1180
Epoch 437/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1161
Epoch 438/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1180
Epoch 439/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1129
Epoch 440/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1156
Epoch 441/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1171
Epoch 442/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1090
Epoch 443/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1062
Epoch 444/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1092
Epoch 445/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1095
Epoch 446/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1112
Epoch 447/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1321
Epoch 448/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1146
Epoch 449/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1121
Epoch 450/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1125
Epoch 451/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1188
Epoch 452/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1086
Epoch 453/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1144
Epoch 454/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1144
Epoch 455/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1253
Epoch 456/1000
202/202 [==============================] - 3s 12ms/step - loss: 0.1140
Epoch 457/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1212
Epoch 458/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1290
Epoch 459/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1306
Epoch 460/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1177
Epoch 461/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1264
Epoch 462/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1163
Epoch 463/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1187
Epoch 464/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1054
Epoch 465/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1255
Epoch 466/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1134
Epoch 467/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1157
Epoch 468/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1240
Epoch 469/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1081
Epoch 470/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1111
Epoch 471/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1364
Epoch 472/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1316
Epoch 473/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1071
Epoch 474/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1111
Epoch 475/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1219
Epoch 476/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1100
Epoch 477/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1380
Epoch 478/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1251
Epoch 479/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1110
Epoch 480/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1117
Epoch 481/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1171
Epoch 482/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1247
Epoch 483/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1081
Epoch 484/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1037
Epoch 485/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1119
Epoch 486/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1080
Epoch 487/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1056
Epoch 488/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1078
Epoch 489/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1042
Epoch 490/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1060
Epoch 491/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1012
Epoch 492/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1142
Epoch 493/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1084
Epoch 494/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1257
Epoch 495/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1113
Epoch 496/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1417
Epoch 497/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1057
Epoch 498/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1115
Epoch 499/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1381
Epoch 500/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1441
Epoch 501/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1251
Epoch 502/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1397
Epoch 503/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1168
Epoch 504/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1071
Epoch 505/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1233
Epoch 506/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1164
Epoch 507/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1039
Epoch 508/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1029
Epoch 509/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1062
Epoch 510/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1254
Epoch 511/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1231
Epoch 512/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1247
Epoch 513/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1141
Epoch 514/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1193
Epoch 515/1000
202/202 [==============================] - 1s 5ms/step - loss: 0.1066
Epoch 516/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1325
Epoch 517/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1119
Epoch 518/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1093
Epoch 519/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1067
Epoch 520/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1213
Epoch 521/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1385
Epoch 522/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1176
Epoch 523/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1294
Epoch 524/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1196
Epoch 525/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1103
Epoch 526/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1123
Epoch 527/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1175
Epoch 528/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1073
Epoch 529/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1166
Epoch 530/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1095
Epoch 531/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1040
Epoch 532/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1065
Epoch 533/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1050
Epoch 534/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1084
Epoch 535/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1152
Epoch 536/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1046
Epoch 537/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1071
Epoch 538/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1032
Epoch 539/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1367
Epoch 540/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1229
Epoch 541/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1059
Epoch 542/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1042
Epoch 543/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1041
Epoch 544/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1083
Epoch 545/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1033
Epoch 546/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1114
Epoch 547/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1120
Epoch 548/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1049
Epoch 549/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1026
Epoch 550/1000
202/202 [==============================] - 2s 10ms/step - loss: 0.1289
Epoch 551/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1094
Epoch 552/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1257
Epoch 553/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1217
Epoch 554/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1050
Epoch 555/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0978
Epoch 556/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1150
Epoch 557/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1166
Epoch 558/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1088
Epoch 559/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1122
Epoch 560/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1221
Epoch 561/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1082
Epoch 562/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1058
Epoch 563/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1080
Epoch 564/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1278
Epoch 565/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1091
Epoch 566/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1025
Epoch 567/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1028
Epoch 568/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1331
Epoch 569/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1262
Epoch 570/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1113
Epoch 571/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1242
Epoch 572/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1044
Epoch 573/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1160
Epoch 574/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1117
Epoch 575/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1010
Epoch 576/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1186
Epoch 577/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1172
Epoch 578/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1338
Epoch 579/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1197
Epoch 580/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1030
Epoch 581/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1066
Epoch 582/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1048
Epoch 583/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0979
Epoch 584/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1010
Epoch 585/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1163
Epoch 586/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1108
Epoch 587/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1059
Epoch 588/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1055
Epoch 589/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1122
Epoch 590/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1064
Epoch 591/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1128
Epoch 592/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1191
Epoch 593/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1060
Epoch 594/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1008
Epoch 595/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0977
Epoch 596/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1017
Epoch 597/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1001
Epoch 598/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1008
Epoch 599/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1625
Epoch 600/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1269
Epoch 601/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1298
Epoch 602/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1153
Epoch 603/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1314
Epoch 604/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1111
Epoch 605/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1109
Epoch 606/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1217
Epoch 607/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1267
Epoch 608/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1206
Epoch 609/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1122
Epoch 610/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1085
Epoch 611/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1041
Epoch 612/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1081
Epoch 613/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1031
Epoch 614/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1077
Epoch 615/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1017
Epoch 616/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0973
Epoch 617/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1005
Epoch 618/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1018
Epoch 619/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1017
Epoch 620/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0986
Epoch 621/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1057
Epoch 622/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1061
Epoch 623/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1007
Epoch 624/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1093
Epoch 625/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1006
Epoch 626/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1013
Epoch 627/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1172
Epoch 628/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1059
Epoch 629/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1025
Epoch 630/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0998
Epoch 631/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0959
Epoch 632/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0978
Epoch 633/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1093
Epoch 634/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1060
Epoch 635/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1416
Epoch 636/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1104
Epoch 637/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1026
Epoch 638/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1135
Epoch 639/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0952
Epoch 640/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0961
Epoch 641/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0975
Epoch 642/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0969
Epoch 643/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0926
Epoch 644/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1038
Epoch 645/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0943
Epoch 646/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0943
Epoch 647/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0997
Epoch 648/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1196
Epoch 649/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0996
Epoch 650/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1131
Epoch 651/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1063
Epoch 652/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1005
Epoch 653/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0967
Epoch 654/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0945
Epoch 655/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0946
Epoch 656/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0967
Epoch 657/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1224
Epoch 658/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1147
Epoch 659/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1089
Epoch 660/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0995
Epoch 661/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0949
Epoch 662/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1025
Epoch 663/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0950
Epoch 664/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0993
Epoch 665/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0967
Epoch 666/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1152
Epoch 667/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0981
Epoch 668/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0937
Epoch 669/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1009
Epoch 670/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1029
Epoch 671/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0961
Epoch 672/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0974
Epoch 673/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1147
Epoch 674/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1074
Epoch 675/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1017
Epoch 676/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0949
Epoch 677/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0942
Epoch 678/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0923
Epoch 679/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0948
Epoch 680/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0945
Epoch 681/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1074
Epoch 682/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1031
Epoch 683/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0954
Epoch 684/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1109
Epoch 685/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1149
Epoch 686/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0969
Epoch 687/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1159
Epoch 688/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1038
Epoch 689/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1008
Epoch 690/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1050
Epoch 691/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0946
Epoch 692/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0936
Epoch 693/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0935
Epoch 694/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1043
Epoch 695/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0940
Epoch 696/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0976
Epoch 697/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1007
Epoch 698/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0955
Epoch 699/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0933
Epoch 700/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1002
Epoch 701/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1175
Epoch 702/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0961
Epoch 703/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0952
Epoch 704/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1046
Epoch 705/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0933
Epoch 706/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1151
Epoch 707/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1146
Epoch 708/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0997
Epoch 709/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1028
Epoch 710/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0933
Epoch 711/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0941
Epoch 712/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1050
Epoch 713/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1068
Epoch 714/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1133
Epoch 715/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0941
Epoch 716/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0920
Epoch 717/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0987
Epoch 718/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1003
Epoch 719/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0978
Epoch 720/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0972
Epoch 721/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0942
Epoch 722/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0916
Epoch 723/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1004
Epoch 724/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0957
Epoch 725/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0891
Epoch 726/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1120
Epoch 727/1000
202/202 [==============================] - 2s 10ms/step - loss: 0.0923
Epoch 728/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1124
Epoch 729/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0943
Epoch 730/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0964
Epoch 731/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1010
Epoch 732/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0922
Epoch 733/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0959
Epoch 734/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1132
Epoch 735/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0947
Epoch 736/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0952
Epoch 737/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0980
Epoch 738/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1119
Epoch 739/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1043
Epoch 740/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0947
Epoch 741/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0947
Epoch 742/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1005
Epoch 743/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0904
Epoch 744/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1261
Epoch 745/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1067
Epoch 746/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0934
Epoch 747/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0918
Epoch 748/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1060
Epoch 749/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1818
Epoch 750/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1455
Epoch 751/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1405
Epoch 752/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1354
Epoch 753/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1385
Epoch 754/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1340
Epoch 755/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1385
Epoch 756/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1290
Epoch 757/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1284
Epoch 758/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1319
Epoch 759/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1281
Epoch 760/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1282
Epoch 761/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1312
Epoch 762/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1329
Epoch 763/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1406
Epoch 764/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1420
Epoch 765/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1288
Epoch 766/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1337
Epoch 767/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1273
Epoch 768/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1315
Epoch 769/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1356
Epoch 770/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1360
Epoch 771/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1267
Epoch 772/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1337
Epoch 773/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1304
Epoch 774/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1246
Epoch 775/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1330
Epoch 776/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1294
Epoch 777/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1322
Epoch 778/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1312
Epoch 779/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1261
Epoch 780/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1267
Epoch 781/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.1424
Epoch 782/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1309
Epoch 783/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1383
Epoch 784/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1286
Epoch 785/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1279
Epoch 786/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1282
Epoch 787/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1264
Epoch 788/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1255
Epoch 789/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1336
Epoch 790/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1330
Epoch 791/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1236
Epoch 792/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1243
Epoch 793/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1267
Epoch 794/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1261
Epoch 795/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1343
Epoch 796/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1257
Epoch 797/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1250
Epoch 798/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1582
Epoch 799/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1297
Epoch 800/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1257
Epoch 801/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1208
Epoch 802/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1185
Epoch 803/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1180
Epoch 804/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1139
Epoch 805/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1134
Epoch 806/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1166
Epoch 807/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1184
Epoch 808/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1065
Epoch 809/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1175
Epoch 810/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1061
Epoch 811/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1037
Epoch 812/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1021
Epoch 813/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1044
Epoch 814/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1029
Epoch 815/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1003
Epoch 816/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1111
Epoch 817/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1092
Epoch 818/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1057
Epoch 819/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1057
Epoch 820/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1042
Epoch 821/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0988
Epoch 822/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1014
Epoch 823/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0995
Epoch 824/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0963
Epoch 825/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1075
Epoch 826/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0968
Epoch 827/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0960
Epoch 828/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0977
Epoch 829/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0954
Epoch 830/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0985
Epoch 831/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0985
Epoch 832/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1042
Epoch 833/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0949
Epoch 834/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0994
Epoch 835/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0951
Epoch 836/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0949
Epoch 837/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0968
Epoch 838/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0926
Epoch 839/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0955
Epoch 840/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0998
Epoch 841/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0949
Epoch 842/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0938
Epoch 843/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0912
Epoch 844/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0949
Epoch 845/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0961
Epoch 846/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0948
Epoch 847/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0933
Epoch 848/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0951
Epoch 849/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0946
Epoch 850/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0941
Epoch 851/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0938
Epoch 852/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0960
Epoch 853/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1001
Epoch 854/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0927
Epoch 855/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0949
Epoch 856/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0933
Epoch 857/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0905
Epoch 858/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0947
Epoch 859/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0982
Epoch 860/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0950
Epoch 861/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0958
Epoch 862/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0936
Epoch 863/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0960
Epoch 864/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0904
Epoch 865/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0911
Epoch 866/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0935
Epoch 867/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0956
Epoch 868/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1017
Epoch 869/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1035
Epoch 870/1000
202/202 [==============================] - 2s 7ms/step - loss: 0.0949
Epoch 871/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0938
Epoch 872/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0906
Epoch 873/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0903
Epoch 874/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0882
Epoch 875/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0925
Epoch 876/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0910
Epoch 877/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0905
Epoch 878/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0918
Epoch 879/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0887
Epoch 880/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1006
Epoch 881/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0959
Epoch 882/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0932
Epoch 883/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0933
Epoch 884/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0959
Epoch 885/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0957
Epoch 886/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0926
Epoch 887/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0948
Epoch 888/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1033
Epoch 889/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0892
Epoch 890/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1022
Epoch 891/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0870
Epoch 892/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0907
Epoch 893/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0890
Epoch 894/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0904
Epoch 895/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0874
Epoch 896/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0909
Epoch 897/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0936
Epoch 898/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1731
Epoch 899/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1388
Epoch 900/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1210
Epoch 901/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1042
Epoch 902/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.1032
Epoch 903/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0960
Epoch 904/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0965
Epoch 905/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0923
Epoch 906/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0959
Epoch 907/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0966
Epoch 908/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0931
Epoch 909/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0946
Epoch 910/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0915
Epoch 911/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0923
Epoch 912/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0987
Epoch 913/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0954
Epoch 914/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0899
Epoch 915/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0908
Epoch 916/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0930
Epoch 917/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0952
Epoch 918/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0963
Epoch 919/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0980
Epoch 920/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0902
Epoch 921/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1066
Epoch 922/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0941
Epoch 923/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0874
Epoch 924/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0879
Epoch 925/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0903
Epoch 926/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0887
Epoch 927/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0914
Epoch 928/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0940
Epoch 929/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0974
Epoch 930/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0891
Epoch 931/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0915
Epoch 932/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0936
Epoch 933/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0881
Epoch 934/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.1116
Epoch 935/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0920
Epoch 936/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1072
Epoch 937/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0939
Epoch 938/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0868
Epoch 939/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0872
Epoch 940/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0944
Epoch 941/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0908
Epoch 942/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0985
Epoch 943/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0904
Epoch 944/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0870
Epoch 945/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0940
Epoch 946/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0962
Epoch 947/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0926
Epoch 948/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1001
Epoch 949/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0888
Epoch 950/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0997
Epoch 951/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0853
Epoch 952/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0873
Epoch 953/1000
202/202 [==============================] - 2s 10ms/step - loss: 0.0903
Epoch 954/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0907
Epoch 955/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0876
Epoch 956/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0925
Epoch 957/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0851
Epoch 958/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0889
Epoch 959/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0927
Epoch 960/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1102
Epoch 961/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0895
Epoch 962/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0977
Epoch 963/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0953
Epoch 964/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0899
Epoch 965/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0917
Epoch 966/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0921
Epoch 967/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0920
Epoch 968/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0958
Epoch 969/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0941
Epoch 970/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0916
Epoch 971/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0849
Epoch 972/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1043
Epoch 973/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0909
Epoch 974/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0855
Epoch 975/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0855
Epoch 976/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0913
Epoch 977/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1022
Epoch 978/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1448
Epoch 979/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1694
Epoch 980/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1223
Epoch 981/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0936
Epoch 982/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0951
Epoch 983/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0892
Epoch 984/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0895
Epoch 985/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0917
Epoch 986/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0912
Epoch 987/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0925
Epoch 988/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1012
Epoch 989/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1002
Epoch 990/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0866
Epoch 991/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0911
Epoch 992/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.1053
Epoch 993/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0870
Epoch 994/1000
202/202 [==============================] - 2s 9ms/step - loss: 0.0960
Epoch 995/1000
202/202 [==============================] - 2s 8ms/step - loss: 0.0874
Epoch 996/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.1062
Epoch 997/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0898
Epoch 998/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0837
Epoch 999/1000
202/202 [==============================] - 1s 7ms/step - loss: 0.0918
Epoch 1000/1000
202/202 [==============================] - 1s 6ms/step - loss: 0.0988
202/202 [==============================] - 1s 2ms/step
Mean absolute error: 0.13608695566654205, Mean absolute error (original): 76802.94667210567
</code></pre>
</div>
</div>
<section id="model-selection--eval" class="cell markdown"
id="ICuqb-a9vfxQ">
<h1>Model selection &amp; eval</h1>
</section>
<div class="cell code" data-execution_count="30"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="vVWR1h-7veaS" data-outputId="61990759-fd9e-47db-fe3f-9583d6921b91">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Z-score</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df.drop([<span class="st">&quot;Weekly_Sales&quot;</span>, <span class="st">&quot;Date&quot;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df.drop([<span class="st">&quot;Weekly_Sales&quot;</span>, <span class="st">&quot;Date&quot;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">0</span>, <span class="dv">1</span>),  <span class="co"># Linear regression (0 hidden layers)</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">2</span>),  <span class="co"># 2-1</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">4</span>),  <span class="co"># 4-1</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="dv">8</span>),  <span class="co"># 8-1</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="dv">8</span>),  <span class="co"># 8-1</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="dv">8</span>),  <span class="co"># 8-8-1</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="dv">16</span>),  <span class="co"># 16-16-16-1</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">16</span>),  <span class="co"># 16-16-16-16-1</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),  <span class="co"># 32-32-32-32-1</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)   <span class="co">#64-64-64-64-1</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint]</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scatter plot for training data</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    plt.scatter(y_train, y_train_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>    plt.plot([y_train.<span class="bu">min</span>(), y_train.<span class="bu">max</span>()], [y_train.<span class="bu">min</span>(), y_train.<span class="bu">max</span>()], <span class="st">&#39;k--&#39;</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&#39;Actual&#39;</span>)</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&#39;Predicted&#39;</span>)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;Training Data: Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scatter plot for validation data</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>    plt.scatter(y_val, y_val_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>    plt.plot([y_val.<span class="bu">min</span>(), y_val.<span class="bu">max</span>()], [y_val.<span class="bu">min</span>(), y_val.<span class="bu">max</span>()], <span class="st">&#39;k--&#39;</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&#39;Actual&#39;</span>)</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&#39;Predicted&#39;</span>)</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;Validation Data: Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # Calculate mean squared error (MSE)</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train_mse = model.evaluate(X_train, y_train, verbose=0)</span></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># val_mse = model.evaluate(X_val, y_val, verbose=0)</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # Calculate mean absolute percentage error (MAPE) - doesn&#39;t work well because values are close to zero, leading to very high error percentages</span></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train_mape = np.mean(np.abs((y_train - y_train_pred.reshape(-1)) / y_train)) * 100</span></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># val_mape = np.mean(np.abs((y_val - y_val_pred.reshape(-1)) / y_val)) * 100</span></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train MSE = {train_mse}, Validation MSE = {val_mse}, Train MAE = {train_mae}, Validation MAE = {val_mae},</span></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>    results_df <span class="op">=</span> results_df.append({<span class="st">&quot;Model&quot;</span>: <span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&quot;</span>,</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">&quot;MAE on Training Set&quot;</span>: train_mae_orig,</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">&quot;MAE on Validation Set&quot;</span>: val_mae_orig},</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>                                ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>results_df</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/ae6259ec773d51e121c445dafdec415a8447320d.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b01b5726db75dcd904b92f4fbadbc484519b157f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (0-1): Train MAE (Orig) = 433271.8859793307, Validation MAE (Orig) = 434836.26951678057
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4655e11a0f6e6f7a8641818a229a78c89f5f4aaa.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/f1371e851b73dccb78bb9405e9fac4a2ee62a9fd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (1-2): Train MAE (Orig) = 430094.2153968089, Validation MAE (Orig) = 428445.6365118086
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/58fb0767134ab1ebe796665249b7109a5a9339c3.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/78e0bc1fbc8686f661c1aeb21f14ae68c03e6f4f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (1-4): Train MAE (Orig) = 402787.99937681307, Validation MAE (Orig) = 407733.0521177751
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/60740eada5b616d88ab5497eb2a833972391fac7.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/e891426d07c93cb1f8c89cd45c0603e557a1f4b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (1-8): Train MAE (Orig) = 393563.90552009945, Validation MAE (Orig) = 397812.109760721
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/c3f45364938561fec3f0b4bb3457f7f93771f6d2.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4e42c3bca1bbf9509227201ea2ba7c0e7e27d44f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (2-8): Train MAE (Orig) = 343817.9398818898, Validation MAE (Orig) = 351107.5221799254
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/816cfdd497d94541c6175a6fb6890a7452dcfa3d.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/73e209b02e0770b32b098ab983d6975dc686ea68.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (3-8): Train MAE (Orig) = 317142.0383930791, Validation MAE (Orig) = 329792.5407527968
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/842df4f6cee92b68ef127dbf948dd75918fb38e8.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/fd4754986b73ea904d049dc85168d4f99cefe282.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (3-16): Train MAE (Orig) = 223078.72746555117, Validation MAE (Orig) = 236902.3283802051
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/f94f2d731e71cd5dd651fe211aff2678fb1fc575.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/5140071a9a4522f63fccd85ddc047ce633aa9ebd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-16): Train MAE (Orig) = 224368.3648466639, Validation MAE (Orig) = 235436.19445463025
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/47d2e35bbb003223f887dc351e34614db2f3dc77.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/4c6d540b88665fd8aa6089e03ffcf66b7fcbf1d4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 170312.36260697263, Validation MAE (Orig) = 182383.74729101927
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/68ad55bda0a72cc2fc51d1a12c1d3258eac439f6.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/83e244aeea87bb6562f4307fd9d583ae0dbf7159.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-64): Train MAE (Orig) = 142070.61427994198, Validation MAE (Orig) = 163007.29542262273
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-30-b04d950d6e12&gt;:117: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({&quot;Model&quot;: f&quot;Model ({num_hidden_layers}-{neurons_per_layer})&quot;,
</code></pre>
</div>
<div class="output execute_result" data-execution_count="30">

  <div id="df-b7764116-87d8-44ae-bb48-5e7ca31e6c0f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>MAE on Training Set</th>
      <th>MAE on Validation Set</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Model (0-1)</td>
      <td>433271.885979</td>
      <td>434836.269517</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Model (1-2)</td>
      <td>430094.215397</td>
      <td>428445.636512</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Model (1-4)</td>
      <td>402787.999377</td>
      <td>407733.052118</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Model (1-8)</td>
      <td>393563.905520</td>
      <td>397812.109761</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Model (2-8)</td>
      <td>343817.939882</td>
      <td>351107.522180</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Model (3-8)</td>
      <td>317142.038393</td>
      <td>329792.540753</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Model (3-16)</td>
      <td>223078.727466</td>
      <td>236902.328380</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Model (4-16)</td>
      <td>224368.364847</td>
      <td>235436.194455</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Model (4-32)</td>
      <td>170312.362607</td>
      <td>182383.747291</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Model (4-64)</td>
      <td>142070.614280</td>
      <td>163007.295423</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-b7764116-87d8-44ae-bb48-5e7ca31e6c0f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-b7764116-87d8-44ae-bb48-5e7ca31e6c0f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-b7764116-87d8-44ae-bb48-5e7ca31e6c0f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="31"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="2fQWo75CvryT" data-outputId="23efd2e7-1dbd-417d-ca06-822622a7e8f2">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_learning_curves(histories, models_to_test):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, (num_hidden_layers, neurons_per_layer) <span class="kw">in</span> <span class="bu">enumerate</span>(models_to_test):</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>        plt.plot(histories[idx].history[<span class="st">&#39;loss&#39;</span>], label<span class="op">=</span><span class="ss">f&#39;Training Loss (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        plt.plot(histories[idx].history[<span class="st">&#39;val_loss&#39;</span>], label<span class="op">=</span><span class="ss">f&#39;Validation Loss (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f&#39;Learning Curves for Model </span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">&#39;Epochs&#39;</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the function to plot the learning curves</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>plot_learning_curves(histories, models_to_test)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/f4b3181b7432fe230115e7f693ecbe5e16a45dc1.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/8c0b9e09e136119b4ad3d8f4f1e0d9f85afea16d.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/07050aa07c1f6dc98eb70dd680e0b4e33cd41ea5.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/0808ea8545690be91d3bd9611e8e9abb44771db9.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b8cfae98e185ff09f449883ef3e936b77934b807.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/2bccac1cad2bfcaa10735b8ec74d30f0493a399c.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/fc86de237233c9092ee7ecd9c3c76b912f643b32.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/277c18f2621d778723eb73cce6415917be078110.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/441ebc80f45f0a269a3d47916d941de72af24934.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/537f004b3a6550cca78ff77e19019ed5bdde7bcf.png" /></p>
</div>
</div>
<section id="finding-feature-importance" class="cell markdown"
id="0U1NYBe4UCNO">
<h1>Finding feature importance</h1>
</section>
<div class="cell code" data-execution_count="39"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="uU2rFgQ4ZMoV" data-outputId="a9d2ce43-ad1c-4c41-a09a-c7adc3689c6e">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;Store&quot;</span>]]</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;Store&quot;</span>]]</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
Model (4-32): Train MAE (Orig) = 261925.2996912557, Validation MAE (Orig) = 279053.8478309509
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-64): Train MAE (Orig) = 321827.69250777044, Validation MAE (Orig) = 333591.7185394655
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="40"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="UoFzDFzOYgCO" data-outputId="3c03d65e-17f2-4840-d473-704aeb562675">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;Holiday_Flag&quot;</span>]]</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;Holiday_Flag&quot;</span>]]</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
Model (4-32): Train MAE (Orig) = 467282.61186282645, Validation MAE (Orig) = 474186.9640988191
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-64): Train MAE (Orig) = 466924.0794674679, Validation MAE (Orig) = 473731.19463020517
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="41"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="hOMFUjnTUApC" data-outputId="3154c7e8-cd26-4ce4-d6ba-f394d432d064">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;Temperature&quot;</span>]]</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;Temperature&quot;</span>]]</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-63"><a href="#cb48-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb48-64"><a href="#cb48-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb48-65"><a href="#cb48-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb48-66"><a href="#cb48-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-67"><a href="#cb48-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb48-68"><a href="#cb48-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb48-69"><a href="#cb48-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb48-70"><a href="#cb48-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb48-71"><a href="#cb48-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb48-72"><a href="#cb48-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-73"><a href="#cb48-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb48-74"><a href="#cb48-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb48-75"><a href="#cb48-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb48-76"><a href="#cb48-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-77"><a href="#cb48-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-32): Train MAE (Orig) = 464281.94236065063, Validation MAE (Orig) = 474923.589313238
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
Model (4-64): Train MAE (Orig) = 463022.82579569, Validation MAE (Orig) = 472920.0953340584
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="42"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="EDyyGy5sYqIF" data-outputId="de39cb26-2ae6-412c-b34e-17e474ca6b57">
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;Fuel_Price&quot;</span>]]</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;Fuel_Price&quot;</span>]]</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-32): Train MAE (Orig) = 466236.79810609197, Validation MAE (Orig) = 473418.6715957116
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-64): Train MAE (Orig) = 465991.14166649396, Validation MAE (Orig) = 472841.3489045991
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="43"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="NUWNMPGoY73I" data-outputId="cf0eb19c-7519-41ea-a2c6-cf076b233e27">
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;CPI&quot;</span>]]</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;CPI&quot;</span>]]</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-63"><a href="#cb52-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb52-64"><a href="#cb52-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb52-65"><a href="#cb52-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb52-66"><a href="#cb52-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-67"><a href="#cb52-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb52-68"><a href="#cb52-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb52-69"><a href="#cb52-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb52-70"><a href="#cb52-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb52-71"><a href="#cb52-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb52-72"><a href="#cb52-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-73"><a href="#cb52-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb52-74"><a href="#cb52-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb52-75"><a href="#cb52-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb52-76"><a href="#cb52-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-77"><a href="#cb52-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
Model (4-32): Train MAE (Orig) = 465611.75452496886, Validation MAE (Orig) = 479586.2112057178
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
Model (4-64): Train MAE (Orig) = 465990.9042333195, Validation MAE (Orig) = 479823.32069919206
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="44"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="VYiDKSVPY_jV" data-outputId="16acaaaa-a472-4f3f-c5f6-51d8155cc45c">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">&quot;Unemployment&quot;</span>]]</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> val_df[[<span class="st">&quot;Unemployment&quot;</span>]]</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">64</span>)</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;MAE on Training Set&quot;</span>, <span class="st">&quot;MAE on Validation Set&quot;</span>])</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and evaluate each model</span></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>        X_train, y_train,</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>        validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the training history to the list</span></span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>    histories.append(history)</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>    train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>    val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>    y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>    y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>    y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>    y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>    train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>    val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-32): Train MAE (Orig) = 439850.75451201823, Validation MAE (Orig) = 457240.64965972654
151/151 [==============================] - 1s 4ms/step
51/51 [==============================] - 0s 2ms/step
Model (4-64): Train MAE (Orig) = 443226.20595524245, Validation MAE (Orig) = 458513.83184275945
</code></pre>
</div>
</div>
<section id="dropping-unimportant-features" class="cell markdown"
id="33e4bDTQbK-D">
<h1>Dropping unimportant features</h1>
</section>
<div class="cell code" data-execution_count="47"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="M0wRRUEabPb_" data-outputId="21203c16-cdac-4f04-ec3a-9694233d92d0">
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>shuffled_df <span class="op">=</span> dfz.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(shuffled_df) <span class="op">*</span> split_ratio)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> shuffled_df.iloc[:split_index]</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> shuffled_df.iloc[split_index:]</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features to drop in each iteration</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>features_to_drop <span class="op">=</span> [<span class="st">&quot;CPI&quot;</span>, <span class="st">&quot;Temperature&quot;</span>, <span class="st">&quot;Holiday_Flag&quot;</span>, <span class="st">&quot;Fuel_Price&quot;</span>, <span class="st">&quot;Unemployment&quot;</span>, <span class="st">&quot;Store&quot;</span>]</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a specified number of hidden layers and neurons per layer</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(num_hidden_layers, neurons_per_layer):</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential()</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>[<span class="bu">len</span>(X_train.columns)]))</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_hidden_layers <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        model.add(layers.Dense(neurons_per_layer, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>))</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model architectures to test</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>models_to_test <span class="op">=</span> [</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">32</span>)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the training history for each model</span></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>histories <span class="op">=</span> []</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature <span class="kw">in</span> features_to_drop:</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare input and output data for training and validation sets</span></span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> train_df.drop([<span class="st">&quot;Weekly_Sales&quot;</span>, <span class="st">&quot;Date&quot;</span>] <span class="op">+</span> features_to_drop[:features_to_drop.index(feature)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> train_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>    X_val <span class="op">=</span> val_df.drop([<span class="st">&quot;Weekly_Sales&quot;</span>, <span class="st">&quot;Date&quot;</span>] <span class="op">+</span> features_to_drop[:features_to_drop.index(feature)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>    y_val <span class="op">=</span> val_df[<span class="st">&quot;Weekly_Sales&quot;</span>]</span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train and evaluate each model</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num_hidden_layers, neurons_per_layer <span class="kw">in</span> models_to_test:</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> create_model(num_hidden_layers, neurons_per_layer)</span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># Default 0.001</span></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>optimizer)</span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>        checkpoint <span class="op">=</span> ModelCheckpoint(<span class="ss">f&quot;best_model_</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">.h5&quot;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>        history <span class="op">=</span> model.fit(</span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>            X_train, y_train,</span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a>            epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>            verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>            validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># callbacks=[checkpoint]</span></span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the training history to the list</span></span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a>        histories.append(history)</span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get predictions</span></span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a>        y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a>        y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get predictions</span></span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a>        y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a>        y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-68"><a href="#cb56-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scatter plot for training data</span></span>
<span id="cb56-69"><a href="#cb56-69" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb56-70"><a href="#cb56-70" aria-hidden="true" tabindex="-1"></a>        plt.scatter(y_train, y_train_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb56-71"><a href="#cb56-71" aria-hidden="true" tabindex="-1"></a>        plt.plot([y_train.<span class="bu">min</span>(), y_train.<span class="bu">max</span>()], [y_train.<span class="bu">min</span>(), y_train.<span class="bu">max</span>()], <span class="st">&#39;k--&#39;</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb56-72"><a href="#cb56-72" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">&#39;Actual&#39;</span>)</span>
<span id="cb56-73"><a href="#cb56-73" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">&#39;Predicted&#39;</span>)</span>
<span id="cb56-74"><a href="#cb56-74" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f&#39;Training Data: Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb56-75"><a href="#cb56-75" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb56-76"><a href="#cb56-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-77"><a href="#cb56-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scatter plot for validation data</span></span>
<span id="cb56-78"><a href="#cb56-78" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb56-79"><a href="#cb56-79" aria-hidden="true" tabindex="-1"></a>        plt.scatter(y_val, y_val_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb56-80"><a href="#cb56-80" aria-hidden="true" tabindex="-1"></a>        plt.plot([y_val.<span class="bu">min</span>(), y_val.<span class="bu">max</span>()], [y_val.<span class="bu">min</span>(), y_val.<span class="bu">max</span>()], <span class="st">&#39;k--&#39;</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb56-81"><a href="#cb56-81" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">&#39;Actual&#39;</span>)</span>
<span id="cb56-82"><a href="#cb56-82" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">&#39;Predicted&#39;</span>)</span>
<span id="cb56-83"><a href="#cb56-83" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f&#39;Validation Data: Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">)&#39;</span>)</span>
<span id="cb56-84"><a href="#cb56-84" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb56-85"><a href="#cb56-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-86"><a href="#cb56-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate mean absolute error (MAE)</span></span>
<span id="cb56-87"><a href="#cb56-87" aria-hidden="true" tabindex="-1"></a>        train_mae <span class="op">=</span> mean_absolute_error(y_train, y_train_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb56-88"><a href="#cb56-88" aria-hidden="true" tabindex="-1"></a>        val_mae <span class="op">=</span> mean_absolute_error(y_val, y_val_pred.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb56-89"><a href="#cb56-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-90"><a href="#cb56-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply inverse transformation to predictions and true target values</span></span>
<span id="cb56-91"><a href="#cb56-91" aria-hidden="true" tabindex="-1"></a>        y_train_pred_orig <span class="op">=</span> (y_train_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb56-92"><a href="#cb56-92" aria-hidden="true" tabindex="-1"></a>        y_train_orig <span class="op">=</span> (y_train <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb56-93"><a href="#cb56-93" aria-hidden="true" tabindex="-1"></a>        y_val_pred_orig <span class="op">=</span> (y_val_pred <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb56-94"><a href="#cb56-94" aria-hidden="true" tabindex="-1"></a>        y_val_orig <span class="op">=</span> (y_val <span class="op">*</span> y_std) <span class="op">+</span> y_mean</span>
<span id="cb56-95"><a href="#cb56-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-96"><a href="#cb56-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate evaluation metrics on the original scale</span></span>
<span id="cb56-97"><a href="#cb56-97" aria-hidden="true" tabindex="-1"></a>        train_mae_orig <span class="op">=</span> mean_absolute_error(y_train_orig, y_train_pred_orig)</span>
<span id="cb56-98"><a href="#cb56-98" aria-hidden="true" tabindex="-1"></a>        val_mae_orig <span class="op">=</span> mean_absolute_error(y_val_orig, y_val_pred_orig)</span>
<span id="cb56-99"><a href="#cb56-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-100"><a href="#cb56-100" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Model (</span><span class="sc">{</span>num_hidden_layers<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>neurons_per_layer<span class="sc">}</span><span class="ss">): Train MAE (Orig) = </span><span class="sc">{</span>train_mae_orig<span class="sc">}</span><span class="ss">, Validation MAE (Orig) = </span><span class="sc">{</span>val_mae_orig<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/45c3168c1eb104a105c4f3c87bfab3a453e7de01.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/5fcab354b3dc1113c224c24d337322a443520e96.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 160799.05368809577, Validation MAE (Orig) = 176682.52503340584
151/151 [==============================] - 1s 2ms/step
51/51 [==============================] - 0s 1ms/step
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 1ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b0494c59d3a060d31d541a97dd3484ce3b7cecdc.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/ebe1d2e2bbb9fdd522b266cae3fc06352faf399c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 222307.8679563821, Validation MAE (Orig) = 252688.83374844625
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/60c4b92bab903891c1942941b9804127cf3d3401.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/6db5960f260885c788f9c7574cb049ddbd33c91f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 229493.21882874018, Validation MAE (Orig) = 247641.61791951524
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/82a14b002e82aaa0ca2a33bb8945c5459a4da1a7.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b2ba3860fb3ce6abac10c051cb33ae2377d2e5c2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 253142.16384039575, Validation MAE (Orig) = 265144.3780282784
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
151/151 [==============================] - 0s 1ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/88d9726da70fa86e9a0e1480a315d5058940fa56.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/dc775ba950a9d72dd2b68223862802f6e98acee5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 280653.9364639453, Validation MAE (Orig) = 291907.72863424494
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
151/151 [==============================] - 0s 2ms/step
51/51 [==============================] - 0s 2ms/step
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/e3f9a0fad633ea0d525398faaa5083497c5e09d8.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_ff7fb5281edf4dec9b50351c5ed05e4f/b4c124bd8f69e32652af165b6b06b1ce034e7137.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Model (4-32): Train MAE (Orig) = 304372.9562520721, Validation MAE (Orig) = 318986.3832908639
</code></pre>
</div>
</div>
</body>
</html>
